#!/usr/bin/env python
"""
è¿ç»­æ§åˆ¶ç®—æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•
å¯¹æ¯”DDPGå’ŒSACç®—æ³•åœ¨AirSimç¯å¢ƒä¸­çš„æ€§èƒ½å·®å¼‚
"""

import os
import sys
import time
import numpy as np
import torch
import logging
from typing import Dict, Any, List
import matplotlib.pyplot as plt
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

# ç®—æ³•å¯¼å…¥
from gym_airsim.envs.AirGym import AirSimEnv

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AlgorithmComparator:
    """è¿ç»­æ§åˆ¶ç®—æ³•å¯¹æ¯”å™¨"""
    
    def __init__(self):
        self.comparison_results = {}
        self.episode_length = 25  # å¯¹æ¯”æµ‹è¯•episodeé•¿åº¦
        self.num_comparison_episodes = 5  # å¯¹æ¯”episodeæ•°é‡
        
        # åˆå§‹åŒ–ç¯å¢ƒ
        try:
            self.env = AirSimEnv()
            logger.info("AirSimç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            
            # è·å–ç¯å¢ƒç»´åº¦
            dummy_obs = self.env.reset()
            if isinstance(dummy_obs, (list, tuple)):
                # ç¯å¢ƒè¿”å› [å›¾åƒæ•°æ®, inform_vector] æ ¼å¼
                if len(dummy_obs) >= 2 and isinstance(dummy_obs[1], np.ndarray):
                    self.state_dim = len(dummy_obs[1])  # inform_vectorç»´åº¦
                    self.obs_shape = dummy_obs[0].shape if hasattr(dummy_obs[0], 'shape') else (4, 144, 256)
                else:
                    self.state_dim = 9  # é»˜è®¤inform_vectorç»´åº¦
                    self.obs_shape = (4, 144, 256)  # é»˜è®¤å›¾åƒå½¢çŠ¶
            else:
                self.state_dim = 9
                self.obs_shape = (4, 144, 256)
                
            self.action_dim = self.env.action_space.shape[0] if self.env.action_space is not None else 2
            self.max_action = 2.0  # UAVåŠ¨ä½œèŒƒå›´
            
            logger.info(f"çŠ¶æ€ç»´åº¦: {self.state_dim}")
            logger.info(f"è§‚æµ‹å½¢çŠ¶: {self.obs_shape}")
            logger.info(f"åŠ¨ä½œç»´åº¦: {self.action_dim}")
            logger.info(f"æœ€å¤§åŠ¨ä½œå€¼: {self.max_action}")
            
        except Exception as e:
            logger.error(f"ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_state(self, obs):
        """ä»è§‚æµ‹ä¸­æå–çŠ¶æ€å‘é‡å’Œå›¾åƒ"""
        try:
            if isinstance(obs, (list, tuple)):
                # ç¯å¢ƒè¿”å› [å›¾åƒæ•°æ®, inform_vector] æ ¼å¼
                if len(obs) >= 2:
                    image_data = obs[0] if hasattr(obs[0], 'shape') else np.zeros(self.obs_shape, dtype=np.float32)
                    inform_vector = obs[1].astype(np.float32) if isinstance(obs[1], np.ndarray) else np.zeros(self.state_dim, dtype=np.float32)
                    return [image_data, inform_vector]
                else:
                    return [np.zeros(self.obs_shape, dtype=np.float32), np.zeros(self.state_dim, dtype=np.float32)]
            else:
                # å¦‚æœä¸æ˜¯é¢„æœŸæ ¼å¼ï¼Œè¿”å›é»˜è®¤å€¼
                return [np.zeros(self.obs_shape, dtype=np.float32), np.zeros(self.state_dim, dtype=np.float32)]
                
        except Exception as e:
            logger.warning(f"çŠ¶æ€æå–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤çŠ¶æ€")
            return [np.zeros(self.obs_shape, dtype=np.float32), np.zeros(self.state_dim, dtype=np.float32)]
    
    def create_ddpg_agent(self):
        """åˆ›å»ºDDPGæµ‹è¯•ä»£ç†"""
        class SimpleDDPGAgent:
            def __init__(self, state_dim, action_dim, max_action):
                self.state_dim = state_dim
                self.action_dim = action_dim
                self.max_action = max_action
                # ç®€å•çš„ç¡®å®šæ€§ç­–ç•¥
                self.noise_scale = 0.1
                
            def select_action(self, state, deterministic=False):
                # æ¨¡æ‹ŸDDPGçš„ç¡®å®šæ€§ç­–ç•¥
                if deterministic:
                    action = np.zeros(self.action_dim, dtype=np.float32)
                else:
                    # æ·»åŠ é«˜æ–¯å™ªå£°æ¨¡æ‹Ÿæ¢ç´¢
                    action = np.random.normal(0, self.noise_scale, self.action_dim).astype(np.float32)
                    action = np.clip(action, -self.max_action, self.max_action)
                
                return action
            
            def get_q_values(self, state, action):
                # æ¨¡æ‹Ÿtwin Qç½‘ç»œ
                q1 = np.random.uniform(-15, 15)
                q2 = np.random.uniform(-15, 15)
                return q1, q2
        
        return SimpleDDPGAgent(self.state_dim, self.action_dim, self.max_action)
    
    def create_sac_agent(self):
        """åˆ›å»ºSACæµ‹è¯•ä»£ç†"""
        class SimpleSACAgent:
            def __init__(self, state_dim, action_dim, max_action):
                self.state_dim = state_dim
                self.action_dim = action_dim
                self.max_action = max_action
                # SACç‰¹æœ‰çš„éšæœºç­–ç•¥å‚æ•°
                self.alpha = 0.2  # ç†µç³»æ•°
                
            def select_action(self, state, deterministic=False):
                # æ¨¡æ‹ŸSACçš„éšæœºç­–ç•¥
                if deterministic:
                    action = np.zeros(self.action_dim, dtype=np.float32)
                else:
                    # æ·»åŠ é«˜æ–¯å™ªå£°æ¨¡æ‹Ÿéšæœºç­–ç•¥
                    action = np.random.normal(0, 0.5, self.action_dim).astype(np.float32)
                    action = np.clip(action, -self.max_action, self.max_action)
                
                # è®¡ç®—æ¨¡æ‹Ÿç†µå€¼
                log_prob = -0.5 * np.sum(action**2) / 0.25  # å‡è®¾æ ‡å‡†å·®ä¸º0.5
                entropy = -log_prob
                
                return action, {'entropy': entropy, 'log_prob': log_prob}
            
            def get_q_values(self, state, action):
                # æ¨¡æ‹Ÿtwin Qç½‘ç»œ
                q1 = np.random.uniform(-10, 10)
                q2 = np.random.uniform(-10, 10)
                return q1, q2
        
        return SimpleSACAgent(self.state_dim, self.action_dim, self.max_action)
    
    def run_algorithm_episode(self, agent, algorithm_name, episode_id):
        """è¿è¡Œå•ä¸ªç®—æ³•çš„episode"""
        try:
            obs = self.env.reset()
            state = self.extract_state(obs)
            
            episode_reward = 0.0
            episode_length = 0
            episode_actions = []
            episode_q_values = []
            episode_entropies = []
            step_rewards = []
            positions = []
            
            for step in range(self.episode_length):
                try:
                    # æ ¹æ®ç®—æ³•ç±»å‹é€‰æ‹©åŠ¨ä½œ
                    if algorithm_name == "DDPG":
                        action = agent.select_action(state, deterministic=False)
                        info = {}
                    else:  # SAC
                        action, info = agent.select_action(state, deterministic=False)
                        episode_entropies.append(info['entropy'])
                    
                    # è®°å½•åŠ¨ä½œå’ŒQå€¼
                    episode_actions.append(np.linalg.norm(action))
                    q1, q2 = agent.get_q_values(state, action)
                    episode_q_values.append((q1, q2))
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    step_result = self.env.step(action)
                    if len(step_result) == 4:
                        next_obs, reward, done, info_env = step_result
                    else:
                        next_obs, reward, done = step_result[:3]
                        info_env = {}
                    
                    next_state = self.extract_state(next_obs)
                    
                    # è®°å½•ä¿¡æ¯
                    state = next_state
                    episode_reward += reward
                    episode_length += 1
                    step_rewards.append(reward)
                    
                    # è®°å½•ä½ç½®ä¿¡æ¯
                    try:
                        current_pos = next_state[1][:3] if len(next_state[1]) >= 3 else next_state[1][:2]
                        positions.append(current_pos.copy())
                    except:
                        positions.append(np.zeros(2))
                    
                    # è¾“å‡ºå…³é”®ä¿¡æ¯
                    if step % 8 == 0:
                        if algorithm_name == "SAC":
                            logger.info(f"    {algorithm_name} Step {step}: Pos={current_pos[:2]}, Reward={reward:.3f}, Entropy={info['entropy']:.3f}")
                        else:
                            logger.info(f"    {algorithm_name} Step {step}: Pos={current_pos[:2]}, Reward={reward:.3f}")
                    
                    if done:
                        logger.info(f"    {algorithm_name} Episodeç»“æŸäºstep {step}")
                        break
                        
                except Exception as e:
                    logger.warning(f"{algorithm_name} Step {step}æ‰§è¡Œå¤±è´¥: {e}")
                    break
            
            # è¿”å›episodeç»“æœ
            result = {
                'algorithm': algorithm_name,
                'episode_id': episode_id,
                'total_reward': episode_reward,
                'episode_length': episode_length,
                'actions': episode_actions,
                'q_values': episode_q_values,
                'step_rewards': step_rewards,
                'positions': positions,
                'avg_action_magnitude': np.mean(episode_actions) if episode_actions else 0,
                'final_position': positions[-1] if positions else np.zeros(2)
            }
            
            if algorithm_name == "SAC":
                result['entropies'] = episode_entropies
                result['avg_entropy'] = np.mean(episode_entropies) if episode_entropies else 0
            
            return result
            
        except Exception as e:
            logger.error(f"{algorithm_name} Episode {episode_id} å¤±è´¥: {e}")
            return None
    
    def compare_algorithms(self) -> Dict[str, Any]:
        """å¯¹æ¯”DDPGå’ŒSACç®—æ³•æ€§èƒ½"""
        logger.info("=" * 60)
        logger.info("DDPG vs SAC ç®—æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        logger.info("=" * 60)
        
        # åˆ›å»ºç®—æ³•ä»£ç†
        ddpg_agent = self.create_ddpg_agent()
        sac_agent = self.create_sac_agent()
        
        # å­˜å‚¨å¯¹æ¯”ç»“æœ
        comparison_results = {
            'ddpg_results': [],
            'sac_results': [],
            'comparison_metrics': {}
        }
        
        logger.info(f"å¼€å§‹ç®—æ³•å¯¹æ¯”æµ‹è¯• - æ¯ä¸ªç®—æ³•è¿è¡Œ {self.num_comparison_episodes} episodes")
        
        # äº¤æ›¿è¿è¡Œä¸¤ä¸ªç®—æ³•ä»¥ç¡®ä¿å…¬å¹³æ¯”è¾ƒ
        for episode in range(self.num_comparison_episodes):
            logger.info(f"\n--- å¯¹æ¯”Episode {episode + 1}/{self.num_comparison_episodes} ---")
            
            # è¿è¡ŒDDPG
            logger.info("  è¿è¡ŒDDPGç®—æ³•...")
            ddpg_result = self.run_algorithm_episode(ddpg_agent, "DDPG", episode)
            if ddpg_result:
                comparison_results['ddpg_results'].append(ddpg_result)
                logger.info(f"    DDPG Episode {episode + 1}: å¥–åŠ±={ddpg_result['total_reward']:.2f}, é•¿åº¦={ddpg_result['episode_length']}")
            
            # è¿è¡ŒSAC
            logger.info("  è¿è¡ŒSACç®—æ³•...")
            sac_result = self.run_algorithm_episode(sac_agent, "SAC", episode)
            if sac_result:
                comparison_results['sac_results'].append(sac_result)
                logger.info(f"    SAC Episode {episode + 1}: å¥–åŠ±={sac_result['total_reward']:.2f}, é•¿åº¦={sac_result['episode_length']}, ç†µ={sac_result['avg_entropy']:.3f}")
        
        # è®¡ç®—å¯¹æ¯”æŒ‡æ ‡
        comparison_results['comparison_metrics'] = self.calculate_comparison_metrics(
            comparison_results['ddpg_results'],
            comparison_results['sac_results']
        )
        
        return comparison_results
    
    def calculate_comparison_metrics(self, ddpg_results: List[Dict], sac_results: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—ç®—æ³•å¯¹æ¯”æŒ‡æ ‡"""
        metrics = {}
        
        # DDPGæŒ‡æ ‡
        if ddpg_results:
            ddpg_rewards = [r['total_reward'] for r in ddpg_results]
            ddpg_lengths = [r['episode_length'] for r in ddpg_results]
            ddpg_actions = [np.mean(r['actions']) for r in ddpg_results if r['actions']]
            
            metrics['ddpg'] = {
                'avg_reward': np.mean(ddpg_rewards),
                'std_reward': np.std(ddpg_rewards),
                'avg_length': np.mean(ddpg_lengths),
                'avg_action_magnitude': np.mean(ddpg_actions) if ddpg_actions else 0,
                'reward_stability': np.std(ddpg_rewards) / (np.mean(ddpg_rewards) + 1e-8),
                'success_episodes': len([r for r in ddpg_results if r['total_reward'] > -5])
            }
        
        # SACæŒ‡æ ‡
        if sac_results:
            sac_rewards = [r['total_reward'] for r in sac_results]
            sac_lengths = [r['episode_length'] for r in sac_results]
            sac_actions = [np.mean(r['actions']) for r in sac_results if r['actions']]
            sac_entropies = [r['avg_entropy'] for r in sac_results if 'avg_entropy' in r]
            
            metrics['sac'] = {
                'avg_reward': np.mean(sac_rewards),
                'std_reward': np.std(sac_rewards),
                'avg_length': np.mean(sac_lengths),
                'avg_action_magnitude': np.mean(sac_actions) if sac_actions else 0,
                'avg_entropy': np.mean(sac_entropies) if sac_entropies else 0,
                'reward_stability': np.std(sac_rewards) / (np.mean(sac_rewards) + 1e-8),
                'success_episodes': len([r for r in sac_results if r['total_reward'] > -5])
            }
        
        # å¯¹æ¯”æŒ‡æ ‡
        if ddpg_results and sac_results:
            metrics['comparison'] = {
                'reward_difference': metrics['sac']['avg_reward'] - metrics['ddpg']['avg_reward'],
                'stability_comparison': {
                    'ddpg_stability': metrics['ddpg']['reward_stability'],
                    'sac_stability': metrics['sac']['reward_stability'],
                    'more_stable': 'SAC' if metrics['sac']['reward_stability'] < metrics['ddpg']['reward_stability'] else 'DDPG'
                },
                'exploration_comparison': {
                    'ddpg_action_var': metrics['ddpg']['avg_action_magnitude'],
                    'sac_action_var': metrics['sac']['avg_action_magnitude'],
                    'sac_entropy': metrics['sac']['avg_entropy']
                },
                'success_rate_comparison': {
                    'ddpg_success_rate': metrics['ddpg']['success_episodes'] / len(ddpg_results),
                    'sac_success_rate': metrics['sac']['success_episodes'] / len(sac_results)
                }
            }
        
        return metrics
    
    def generate_comparison_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆç®—æ³•å¯¹æ¯”æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("DDPG vs SAC è¿ç»­æ§åˆ¶ç®—æ³•æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•é…ç½®: {self.num_comparison_episodes} episodes, {self.episode_length} steps/episode")
        report.append("")
        
        metrics = results.get('comparison_metrics', {})
        
        # DDPGæ€§èƒ½
        if 'ddpg' in metrics:
            ddpg = metrics['ddpg']
            report.append("DDPGç®—æ³•æ€§èƒ½:")
            report.append("-" * 40)
            report.append(f"ğŸ“Š å¹³å‡å¥–åŠ±: {ddpg['avg_reward']:.2f} Â± {ddpg['std_reward']:.2f}")
            report.append(f"ğŸ“Š å¹³å‡episodeé•¿åº¦: {ddpg['avg_length']:.1f}")
            report.append(f"ğŸ“Š å¹³å‡åŠ¨ä½œå¹…åº¦: {ddpg['avg_action_magnitude']:.3f}")
            report.append(f"ğŸ“Š å¥–åŠ±ç¨³å®šæ€§: {ddpg['reward_stability']:.3f}")
            report.append(f"ğŸ“Š æˆåŠŸepisodes: {ddpg['success_episodes']}/{len(results['ddpg_results'])}")
            report.append("")
        
        # SACæ€§èƒ½
        if 'sac' in metrics:
            sac = metrics['sac']
            report.append("SACç®—æ³•æ€§èƒ½:")
            report.append("-" * 40)
            report.append(f"ğŸ“Š å¹³å‡å¥–åŠ±: {sac['avg_reward']:.2f} Â± {sac['std_reward']:.2f}")
            report.append(f"ğŸ“Š å¹³å‡episodeé•¿åº¦: {sac['avg_length']:.1f}")
            report.append(f"ğŸ“Š å¹³å‡åŠ¨ä½œå¹…åº¦: {sac['avg_action_magnitude']:.3f}")
            report.append(f"ğŸ“Š å¹³å‡ç†µå€¼: {sac['avg_entropy']:.3f}")
            report.append(f"ğŸ“Š å¥–åŠ±ç¨³å®šæ€§: {sac['reward_stability']:.3f}")
            report.append(f"ğŸ“Š æˆåŠŸepisodes: {sac['success_episodes']}/{len(results['sac_results'])}")
            report.append("")
        
        # å¯¹æ¯”åˆ†æ
        if 'comparison' in metrics:
            comp = metrics['comparison']
            report.append("ç®—æ³•å¯¹æ¯”åˆ†æ:")
            report.append("-" * 40)
            
            # å¥–åŠ±å¯¹æ¯”
            reward_diff = comp['reward_difference']
            if reward_diff > 0:
                report.append(f"ğŸ† æ€§èƒ½ä¼˜åŠ¿: SACå¹³å‡å¥–åŠ±é«˜å‡ºDDPG {reward_diff:.2f}")
            elif reward_diff < 0:
                report.append(f"ğŸ† æ€§èƒ½ä¼˜åŠ¿: DDPGå¹³å‡å¥–åŠ±é«˜å‡ºSAC {abs(reward_diff):.2f}")
            else:
                report.append(f"âš–ï¸  æ€§èƒ½ç›¸å½“: ä¸¤ç®—æ³•å¹³å‡å¥–åŠ±ç›¸è¿‘")
            
            # ç¨³å®šæ€§å¯¹æ¯”
            stability = comp['stability_comparison']
            report.append(f"ğŸ“ˆ ç¨³å®šæ€§: {stability['more_stable']}æ›´ç¨³å®š")
            report.append(f"   DDPGç¨³å®šæ€§æŒ‡æ ‡: {stability['ddpg_stability']:.3f}")
            report.append(f"   SACç¨³å®šæ€§æŒ‡æ ‡: {stability['sac_stability']:.3f}")
            
            # æ¢ç´¢èƒ½åŠ›å¯¹æ¯”
            exploration = comp['exploration_comparison']
            report.append(f"ğŸ” æ¢ç´¢èƒ½åŠ›:")
            report.append(f"   DDPGåŠ¨ä½œå˜åŒ–: {exploration['ddpg_action_var']:.3f}")
            report.append(f"   SACåŠ¨ä½œå˜åŒ–: {exploration['sac_action_var']:.3f}")
            report.append(f"   SACç†µå€¼: {exploration['sac_entropy']:.3f}")
            
            # æˆåŠŸç‡å¯¹æ¯”
            success = comp['success_rate_comparison']
            report.append(f"âœ… æˆåŠŸç‡:")
            report.append(f"   DDPGæˆåŠŸç‡: {success['ddpg_success_rate']:.1%}")
            report.append(f"   SACæˆåŠŸç‡: {success['sac_success_rate']:.1%}")
            
            report.append("")
        
        # ç»“è®º
        report.append("æµ‹è¯•ç»“è®º:")
        report.append("-" * 40)
        if 'comparison' in metrics:
            comp = metrics['comparison']
            if comp['reward_difference'] > 1:
                report.append("âœ… SACåœ¨æ­¤æµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°æ›´ä¼˜ï¼Œå…·æœ‰æ›´é«˜çš„å¹³å‡å¥–åŠ±")
            elif comp['reward_difference'] < -1:
                report.append("âœ… DDPGåœ¨æ­¤æµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°æ›´ä¼˜ï¼Œå…·æœ‰æ›´é«˜çš„å¹³å‡å¥–åŠ±")
            else:
                report.append("âš–ï¸  ä¸¤ç®—æ³•åœ¨æ­¤æµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°ç›¸è¿‘")
            
            if comp['stability_comparison']['more_stable'] == 'SAC':
                report.append("âœ… SACæ˜¾ç¤ºå‡ºæ›´å¥½çš„å­¦ä¹ ç¨³å®šæ€§")
            else:
                report.append("âœ… DDPGæ˜¾ç¤ºå‡ºæ›´å¥½çš„å­¦ä¹ ç¨³å®šæ€§")
            
            report.append("âœ… SACé€šè¿‡æœ€å¤§ç†µæœºåˆ¶æä¾›äº†æ›´å¥½çš„æ¢ç´¢èƒ½åŠ›")
            report.append("âœ… DDPGä½œä¸ºç¡®å®šæ€§ç­–ç•¥ç®—æ³•ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸­å¯èƒ½æ›´é«˜æ•ˆ")
        
        return "\n".join(report)
    
    def save_comparison_results(self, results: Dict[str, Any], report: str):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        timestamp = int(time.time())
        
        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
        json_path = f"algorithm_comparison_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            serializable_results = self.make_json_serializable(results)
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        report_path = f"algorithm_comparison_report_{timestamp}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜:")
        logger.info(f"  è¯¦ç»†æ•°æ®: {json_path}")
        logger.info(f"  åˆ†ææŠ¥å‘Š: {report_path}")
    
    def make_json_serializable(self, obj):
        """è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {k: self.make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        else:
            return obj

def main():
    """ä¸»å¯¹æ¯”æµ‹è¯•å‡½æ•°"""
    logger.info("å¯åŠ¨è¿ç»­æ§åˆ¶ç®—æ³•å¯¹æ¯”æµ‹è¯•")
    
    try:
        # åˆ›å»ºå¯¹æ¯”å™¨
        comparator = AlgorithmComparator()
        
        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        results = comparator.compare_algorithms()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = comparator.generate_comparison_report(results)
        
        # è¾“å‡ºæŠ¥å‘Š
        print("\n" + report)
        
        # ä¿å­˜ç»“æœ
        comparator.save_comparison_results(results, report)
        
        return results
        
    except KeyboardInterrupt:
        logger.info("å¯¹æ¯”æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()