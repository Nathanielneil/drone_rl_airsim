#!/usr/bin/env python
"""
DDPGç®—æ³•åŠŸèƒ½æµ‹è¯•
éªŒè¯Twin Criticæ¶æ„å’Œè¿ç»­æ§åˆ¶æ€§èƒ½
"""

import os
import sys
import time
import numpy as np
import torch
import logging
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

# ç®—æ³•å¯¼å…¥
from gym_airsim.envs.AirGym import AirSimEnv

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DDPGConfig:
    """DDPGé…ç½®ç±»"""
    def __init__(self):
        self.actor_lr = 1e-4
        self.critic_lr = 1e-3
        self.gamma = 0.99
        self.tau = 0.005
        self.batch_size = 64
        self.buffer_size = 1000000
        self.noise_std = 0.1
        self.policy_noise = 0.2
        self.noise_clip = 0.5
        self.policy_freq = 2

class DDPGTester:
    """DDPGç®—æ³•æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.episode_length = 20  # è¾ƒçŸ­çš„æµ‹è¯•episode
        self.num_test_episodes = 3
        
        # åˆå§‹åŒ–ç¯å¢ƒ
        try:
            self.env = AirSimEnv()
            logger.info("AirSimç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            
            # è·å–ç¯å¢ƒç»´åº¦
            dummy_obs = self.env.reset()
            if isinstance(dummy_obs, dict):
                # å¤„ç†å­—å…¸æ ¼å¼è§‚æµ‹
                if 'inform_vector' in dummy_obs:
                    self.state_dim = len(dummy_obs['inform_vector'])
                elif 'observation' in dummy_obs:
                    self.state_dim = len(dummy_obs['observation'])
                else:
                    self.state_dim = 109  # é»˜è®¤å€¼
            elif isinstance(dummy_obs, (list, tuple)):
                # ç¯å¢ƒè¿”å› [å›¾åƒæ•°æ®, inform_vector] æ ¼å¼
                if len(dummy_obs) >= 2 and isinstance(dummy_obs[1], np.ndarray):
                    self.state_dim = len(dummy_obs[1])  # inform_vectorç»´åº¦
                else:
                    self.state_dim = 9  # é»˜è®¤inform_vectorç»´åº¦
            else:
                self.state_dim = len(dummy_obs) if dummy_obs is not None else 9
                
            self.action_dim = self.env.action_space.shape[0] if self.env.action_space is not None else 2
            self.max_action = 2.0  # UAVåŠ¨ä½œèŒƒå›´
            
            logger.info(f"çŠ¶æ€ç»´åº¦: {self.state_dim}")
            logger.info(f"åŠ¨ä½œç»´åº¦: {self.action_dim}")
            logger.info(f"æœ€å¤§åŠ¨ä½œå€¼: {self.max_action}")
            
        except Exception as e:
            logger.error(f"ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_state(self, obs):
        """ä»è§‚æµ‹ä¸­æå–çŠ¶æ€å‘é‡"""
        try:
            if isinstance(obs, dict):
                if 'inform_vector' in obs:
                    state = obs['inform_vector']
                elif 'observation' in obs:
                    state = obs['observation']
                else:
                    # å¦‚æœæ˜¯å…¶ä»–å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–æ•°å€¼
                    values = []
                    for key, value in obs.items():
                        if isinstance(value, (int, float)):
                            values.append(value)
                        elif isinstance(value, (list, np.ndarray)):
                            if isinstance(value, np.ndarray):
                                values.extend(value.flatten())
                            else:
                                values.extend(value)
                    state = np.array(values[:self.state_dim], dtype=np.float32)
                    return state
            elif isinstance(obs, (list, tuple)):
                # ç¯å¢ƒè¿”å› [å›¾åƒæ•°æ®, inform_vector] æ ¼å¼
                if len(obs) >= 2 and isinstance(obs[1], np.ndarray):
                    # ä½¿ç”¨inform_vectorä½œä¸ºçŠ¶æ€
                    state = obs[1].astype(np.float32)
                else:
                    # å¦‚æœç¬¬äºŒä¸ªå…ƒç´ ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ•°ç»„ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ª
                    state = obs[0] if len(obs) > 0 else np.zeros(self.state_dim, dtype=np.float32)
            elif isinstance(obs, np.ndarray):
                state = obs.astype(np.float32)
            else:
                logger.warning(f"æœªçŸ¥è§‚æµ‹æ ¼å¼: {type(obs)}")
                return np.zeros(self.state_dim, dtype=np.float32)
            
            # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ•°æ®ç±»å‹å’Œç»´åº¦
            if isinstance(state, np.ndarray):
                state = state.astype(np.float32).flatten()
                if len(state) > self.state_dim:
                    state = state[:self.state_dim]
                elif len(state) < self.state_dim:
                    # å¡«å……åˆ°æ­£ç¡®ç»´åº¦
                    padded_state = np.zeros(self.state_dim, dtype=np.float32)
                    padded_state[:len(state)] = state
                    state = padded_state
            else:
                state = np.array([state], dtype=np.float32)
                
            return state
            
        except Exception as e:
            logger.warning(f"çŠ¶æ€æå–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤çŠ¶æ€")
            return np.zeros(self.state_dim, dtype=np.float32)
    
    def test_ddpg_basic_functionality(self) -> Dict[str, Any]:
        """æµ‹è¯•DDPGåŸºæœ¬åŠŸèƒ½"""
        logger.info("="*60)
        logger.info("æµ‹è¯• DDPG åŸºæœ¬åŠŸèƒ½")
        logger.info("="*60)
        
        # åŠ¨æ€å¯¼å…¥DDPG
        try:
            from ddpg import DDPG, OUNoise
        except ImportError as e:
            logger.error(f"DDPGå¯¼å…¥å¤±è´¥: {e}")
            return {'status': 'FAILED', 'error': 'Import failed'}
        
        config = DDPGConfig()
        
        # åˆå§‹åŒ–DDPG agent
        try:
            agent = DDPG(self.state_dim, self.action_dim, self.max_action, config)
            noise = OUNoise(self.action_dim)
            logger.info("DDPG agentåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"DDPGåˆå§‹åŒ–å¤±è´¥: {e}")
            return {'status': 'FAILED', 'error': str(e)}
        
        results = {
            'algorithm': 'DDPG',
            'episodes_completed': 0,
            'episode_rewards': [],
            'episode_lengths': [],
            'actor_losses': [],
            'critic_losses': [],
            'exploration_noise': [],
            'action_magnitudes': [],
            'twin_critic_verified': False,
            'ou_noise_verified': False
        }
        
        logger.info(f"å¼€å§‹DDPGåŸºæœ¬åŠŸèƒ½æµ‹è¯• - {self.num_test_episodes}ä¸ªepisode")
        
        for episode in range(self.num_test_episodes):
            logger.info(f"\n--- DDPG Episode {episode + 1}/{self.num_test_episodes} ---")
            
            try:
                # é‡ç½®ç¯å¢ƒå’Œå™ªå£°
                obs = self.env.reset()
                state = self.extract_state(obs)
                noise.reset()
                
                episode_reward = 0.0
                episode_length = 0
                episode_actions = []
                episode_noise = []
                
                for step in range(self.episode_length):
                    try:
                        # é€‰æ‹©åŠ¨ä½œ (æµ‹è¯•é˜¶æ®µï¼Œæ·»åŠ å™ªå£°ç”¨äºæ¢ç´¢)
                        with torch.no_grad():
                            state_tensor = torch.FloatTensor(state).unsqueeze(0)
                            if hasattr(agent, 'device'):
                                state_tensor = state_tensor.to(agent.device)
                            
                            # è·å–ç¡®å®šæ€§åŠ¨ä½œ
                            action = agent.actor(state_tensor).cpu().numpy().flatten()
                            
                            # æ·»åŠ OUå™ªå£°
                            noise_sample = noise.noise()
                            action = action + noise_sample
                            action = np.clip(action, -self.max_action, self.max_action)
                            
                            episode_actions.append(np.linalg.norm(action))
                            episode_noise.append(np.linalg.norm(noise_sample))
                        
                        # æ‰§è¡ŒåŠ¨ä½œ
                        step_result = self.env.step(action)
                        if len(step_result) == 4:
                            next_obs, reward, done, info = step_result
                        else:
                            next_obs, reward, done = step_result[:3]
                            info = {}
                        
                        next_state = self.extract_state(next_obs)
                        
                        # å­˜å‚¨ç»éªŒåˆ°replay buffer (å¦‚æœæœ‰çš„è¯)
                        if hasattr(agent, 'replay_buffer'):
                            agent.replay_buffer.add(state, action, next_state, reward, done)
                        
                        # æ›´æ–°çŠ¶æ€
                        state = next_state
                        episode_reward += reward
                        episode_length += 1
                        
                        # è¾“å‡ºå…³é”®ä¿¡æ¯
                        if step % 5 == 0:
                            current_pos = next_state[:3] if len(next_state) >= 3 else next_state[:2]
                            logger.info(f"  Step {step}: Pos={current_pos[:2]}, Reward={reward:.3f}, Action_norm={np.linalg.norm(action):.3f}")
                        
                        if done:
                            logger.info(f"  Episodeç»“æŸäºstep {step}")
                            break
                            
                    except Exception as e:
                        logger.warning(f"Step {step}æ‰§è¡Œå¤±è´¥: {e}")
                        break
                
                # æµ‹è¯•Twin Criticæ¶æ„
                try:
                    if len(episode_actions) > 0:
                        # æµ‹è¯•åŒQç½‘ç»œ
                        state_tensor = torch.FloatTensor(state).unsqueeze(0)
                        action_tensor = torch.FloatTensor(action).unsqueeze(0)
                        if hasattr(agent, 'device'):
                            state_tensor = state_tensor.to(agent.device)
                            action_tensor = action_tensor.to(agent.device)
                        
                        with torch.no_grad():
                            q1, q2 = agent.critic(state_tensor, action_tensor)
                            if q1.shape == q2.shape and q1.numel() > 0 and q2.numel() > 0:
                                results['twin_critic_verified'] = True
                                logger.info(f"  Twin CriticéªŒè¯æˆåŠŸ: Q1={q1.item():.3f}, Q2={q2.item():.3f}")
                except Exception as e:
                    logger.warning(f"Twin Criticæµ‹è¯•å¤±è´¥: {e}")
                
                # è®°å½•ç»“æœ
                results['episodes_completed'] += 1
                results['episode_rewards'].append(episode_reward)
                results['episode_lengths'].append(episode_length)
                results['action_magnitudes'].extend(episode_actions)
                results['exploration_noise'].extend(episode_noise)
                
                if len(episode_noise) > 0:
                    results['ou_noise_verified'] = True
                
                logger.info(f"  Episode {episode + 1} å®Œæˆ:")
                logger.info(f"    å¥–åŠ±: {episode_reward:.2f}")
                logger.info(f"    é•¿åº¦: {episode_length}")
                logger.info(f"    å¹³å‡åŠ¨ä½œå¹…åº¦: {np.mean(episode_actions):.3f}")
                logger.info(f"    å¹³å‡å™ªå£°å¹…åº¦: {np.mean(episode_noise):.3f}")
                
            except Exception as e:
                logger.error(f"Episode {episode + 1} å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if results['episodes_completed'] > 0:
            results['avg_reward'] = np.mean(results['episode_rewards'])
            results['avg_length'] = np.mean(results['episode_lengths'])
            results['avg_action_magnitude'] = np.mean(results['action_magnitudes'])
            results['avg_noise_magnitude'] = np.mean(results['exploration_noise'])
        else:
            results['avg_reward'] = 0
            results['avg_length'] = 0
            results['avg_action_magnitude'] = 0
            results['avg_noise_magnitude'] = 0
        
        logger.info(f"\nDDPGåŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆ:")
        logger.info(f"  å®Œæˆepisodes: {results['episodes_completed']}")
        logger.info(f"  å¹³å‡å¥–åŠ±: {results['avg_reward']:.2f}")
        logger.info(f"  å¹³å‡é•¿åº¦: {results['avg_length']:.1f}")
        logger.info(f"  Twin CriticéªŒè¯: {'âœ“' if results['twin_critic_verified'] else 'âœ—'}")
        logger.info(f"  OUå™ªå£°éªŒè¯: {'âœ“' if results['ou_noise_verified'] else 'âœ—'}")
        logger.info(f"  å¹³å‡åŠ¨ä½œå¹…åº¦: {results['avg_action_magnitude']:.3f}")
        logger.info(f"  å¹³å‡å™ªå£°å¹…åº¦: {results['avg_noise_magnitude']:.3f}")
        
        return results
    
    def test_ddpg_network_architecture(self) -> Dict[str, Any]:
        """æµ‹è¯•DDPGç½‘ç»œæ¶æ„ç‰¹æ€§"""
        logger.info("="*60)
        logger.info("æµ‹è¯• DDPG ç½‘ç»œæ¶æ„ç‰¹æ€§")
        logger.info("="*60)
        
        try:
            from ddpg import DDPG, Actor, Critic
        except ImportError as e:
            logger.error(f"DDPGå¯¼å…¥å¤±è´¥: {e}")
            return {'status': 'FAILED', 'error': 'Import failed'}
        
        config = DDPGConfig()
        
        results = {
            'actor_architecture_verified': False,
            'critic_architecture_verified': False,
            'target_networks_verified': False,
            'parameter_counts': {},
            'network_outputs': {}
        }
        
        try:
            # æµ‹è¯•Actorç½‘ç»œ
            actor = Actor(self.state_dim, self.action_dim, self.max_action)
            
            # è®¡ç®—å‚æ•°æ•°é‡
            actor_params = sum(p.numel() for p in actor.parameters())
            results['parameter_counts']['actor'] = actor_params
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            dummy_state = torch.randn(1, self.state_dim)
            actor_output = actor(dummy_state)
            
            if actor_output.shape == (1, self.action_dim):
                results['actor_architecture_verified'] = True
                results['network_outputs']['actor_shape'] = actor_output.shape
                logger.info(f"Actorç½‘ç»œéªŒè¯æˆåŠŸ: è¾“å‡ºå½¢çŠ¶ {actor_output.shape}, å‚æ•°æ•°é‡ {actor_params}")
            
            # æµ‹è¯•Criticç½‘ç»œ (Twin Critic)
            critic = Critic(self.state_dim, self.action_dim)
            
            critic_params = sum(p.numel() for p in critic.parameters())
            results['parameter_counts']['critic'] = critic_params
            
            # æµ‹è¯•Twin Criticè¾“å‡º
            dummy_action = torch.randn(1, self.action_dim)
            q1, q2 = critic(dummy_state, dummy_action)
            
            if q1.shape == (1, 1) and q2.shape == (1, 1):
                results['critic_architecture_verified'] = True
                results['network_outputs']['critic_q1_shape'] = q1.shape
                results['network_outputs']['critic_q2_shape'] = q2.shape
                logger.info(f"Criticç½‘ç»œéªŒè¯æˆåŠŸ: Q1å½¢çŠ¶ {q1.shape}, Q2å½¢çŠ¶ {q2.shape}, å‚æ•°æ•°é‡ {critic_params}")
            
            # æµ‹è¯•å®Œæ•´DDPG agentçš„targetç½‘ç»œ
            agent = DDPG(self.state_dim, self.action_dim, self.max_action, config)
            
            # éªŒè¯targetç½‘ç»œå­˜åœ¨
            if hasattr(agent, 'actor_target') and hasattr(agent, 'critic_target'):
                results['target_networks_verified'] = True
                logger.info("Targetç½‘ç»œéªŒè¯æˆåŠŸ")
                
                # æµ‹è¯•è½¯æ›´æ–°æœºåˆ¶
                original_actor_param = list(agent.actor.parameters())[0].clone()
                original_target_param = list(agent.actor_target.parameters())[0].clone()
                
                # æ‰§è¡Œä¸€æ¬¡è½¯æ›´æ–° (å¦‚æœæœ‰è¯¥æ–¹æ³•)
                if hasattr(agent, 'soft_update'):
                    agent.soft_update()
                    updated_target_param = list(agent.actor_target.parameters())[0]
                    
                    # æ£€æŸ¥å‚æ•°æ˜¯å¦æœ‰å¾®å°å˜åŒ–
                    if not torch.equal(original_target_param, updated_target_param):
                        logger.info("è½¯æ›´æ–°æœºåˆ¶éªŒè¯æˆåŠŸ")
                        results['soft_update_verified'] = True
            
        except Exception as e:
            logger.error(f"ç½‘ç»œæ¶æ„æµ‹è¯•å¤±è´¥: {e}")
            results['error'] = str(e)
        
        logger.info(f"\nDDPGç½‘ç»œæ¶æ„æµ‹è¯•å®Œæˆ:")
        logger.info(f"  Actoræ¶æ„: {'âœ“' if results['actor_architecture_verified'] else 'âœ—'}")
        logger.info(f"  Criticæ¶æ„: {'âœ“' if results['critic_architecture_verified'] else 'âœ—'}")
        logger.info(f"  Targetç½‘ç»œ: {'âœ“' if results['target_networks_verified'] else 'âœ—'}")
        
        return results
    
    def run_all_tests(self) -> Dict[str, Dict[str, Any]]:
        """è¿è¡Œæ‰€æœ‰DDPGæµ‹è¯•"""
        logger.info("å¼€å§‹DDPGç®—æ³•ç»¼åˆæµ‹è¯•")
        logger.info(f"æ¯ä¸ªæµ‹è¯•åŒ…å« {self.num_test_episodes} episodesï¼Œæ¯episodeæœ€å¤š {self.episode_length} steps")
        
        all_results = {}
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        try:
            logger.info("\nå¼€å§‹æµ‹è¯•DDPGåŸºæœ¬åŠŸèƒ½...")
            all_results['basic_functionality'] = self.test_ddpg_basic_functionality()
        except Exception as e:
            logger.error(f"DDPGåŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            all_results['basic_functionality'] = {'status': 'FAILED', 'error': str(e)}
        
        # æµ‹è¯•ç½‘ç»œæ¶æ„
        try:
            logger.info("\nå¼€å§‹æµ‹è¯•DDPGç½‘ç»œæ¶æ„...")
            all_results['network_architecture'] = self.test_ddpg_network_architecture()
        except Exception as e:
            logger.error(f"DDPGç½‘ç»œæ¶æ„æµ‹è¯•å¤±è´¥: {e}")
            all_results['network_architecture'] = {'status': 'FAILED', 'error': str(e)}
        
        return all_results
    
    def generate_test_report(self, results: Dict[str, Dict[str, Any]]) -> str:
        """ç”ŸæˆDDPGæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("DDPG (Deep Deterministic Policy Gradient) æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•é…ç½®: {self.num_test_episodes} episodes, {self.episode_length} steps/episode")
        report.append("")
        
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•ç»“æœ
        if 'basic_functionality' in results:
            basic = results['basic_functionality']
            report.append("åŸºæœ¬åŠŸèƒ½æµ‹è¯•ç»“æœ:")
            report.append("-" * 40)
            
            if 'status' in basic and basic['status'] == 'FAILED':
                report.append(f"âŒ æµ‹è¯•å¤±è´¥: {basic.get('error', 'Unknown error')}")
            else:
                report.append(f"âœ… å®Œæˆepisodes: {basic.get('episodes_completed', 0)}")
                report.append(f"âœ… å¹³å‡å¥–åŠ±: {basic.get('avg_reward', 0):.2f}")
                report.append(f"âœ… å¹³å‡episodeé•¿åº¦: {basic.get('avg_length', 0):.1f}")
                report.append(f"{'âœ…' if basic.get('twin_critic_verified') else 'âŒ'} Twin Criticæ¶æ„éªŒè¯")
                report.append(f"{'âœ…' if basic.get('ou_noise_verified') else 'âŒ'} OUå™ªå£°æœºåˆ¶éªŒè¯")
                report.append(f"âœ… å¹³å‡åŠ¨ä½œå¹…åº¦: {basic.get('avg_action_magnitude', 0):.3f}")
                report.append(f"âœ… å¹³å‡å™ªå£°å¹…åº¦: {basic.get('avg_noise_magnitude', 0):.3f}")
            report.append("")
        
        # ç½‘ç»œæ¶æ„æµ‹è¯•ç»“æœ
        if 'network_architecture' in results:
            arch = results['network_architecture']
            report.append("ç½‘ç»œæ¶æ„æµ‹è¯•ç»“æœ:")
            report.append("-" * 40)
            
            if 'status' in arch and arch['status'] == 'FAILED':
                report.append(f"âŒ æµ‹è¯•å¤±è´¥: {arch.get('error', 'Unknown error')}")
            else:
                report.append(f"{'âœ…' if arch.get('actor_architecture_verified') else 'âŒ'} Actorç½‘ç»œæ¶æ„")
                report.append(f"{'âœ…' if arch.get('critic_architecture_verified') else 'âŒ'} Criticç½‘ç»œæ¶æ„")
                report.append(f"{'âœ…' if arch.get('target_networks_verified') else 'âŒ'} Targetç½‘ç»œæœºåˆ¶")
                
                if 'parameter_counts' in arch:
                    params = arch['parameter_counts']
                    report.append(f"ğŸ“Š Actorå‚æ•°æ•°é‡: {params.get('actor', 0):,}")
                    report.append(f"ğŸ“Š Criticå‚æ•°æ•°é‡: {params.get('critic', 0):,}")
            report.append("")
        
        # æ€»ç»“
        report.append("æµ‹è¯•æ€»ç»“:")
        report.append("-" * 40)
        total_tests = len(results)
        passed_tests = sum(1 for r in results.values() if not (isinstance(r, dict) and r.get('status') == 'FAILED'))
        report.append(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        report.append(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
        report.append(f"æµ‹è¯•é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
        
        return "\n".join(report)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¯åŠ¨DDPGç®—æ³•æµ‹è¯•")
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = DDPGTester()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        results = tester.run_all_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_test_report(results)
        
        # è¾“å‡ºæŠ¥å‘Š
        print("\n" + report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = f"ddpg_test_report_{int(time.time())}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"DDPGæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return results
        
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()